[
  {
    "objectID": "Templates/DS350_Template.html",
    "href": "Templates/DS350_Template.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "TODO: Update with template from Paul\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Xave Perry ’s Resume",
    "section": "",
    "text": "Computer Science Major at Brigham Young University - Idaho\n\nperrshan2001@byui.edu | Data Science Program | LinkedIn | GitHub\n\n\n\n\n2017-2020 Idaho State University, Pocatello, ID\n\nA.A in General Studies with a 3.89 GPA\n\nExpected 2025 Brigham Young University - Idaho, Rexburg, ID\n\nBS degree in Computer Science with a 3.80 GPA\nMinor in Computer Engineering\n\n\n\n\n\nLanguages: Python, C, C#, MySQL, Erlang, Javascript, HTML, CSS, Rust\nSoftware Tools: Visual Studio Code, Tableau, Logism-Evolution, LTspice, STM Cube IDE, Eclipse\n\n\n\n\n\nDesigned a slide puzzle using C and a nucleo-I476 board\nCreated a 4-bit CPU to do basic math and display characters and numbers\nMade an object-oriented banking program in C# using abstraction, encapsulation, inheritance, and polymorphism\nCreated a basic network game that multiple players can connect to a play using Python and TCP connections.\n\n\n\n\n2023-2024 Customer Care Representative, Melaleuca\n\nAchieved Star Performer amongst all of the other customer care representatives\nEarned a score within the top 20% of the other agents two months in a row\nChosen to be one of the few agents to take extra calls on the Riverbend Ranch phone line\nImproved my average handle time increasing my volume of phone calls so that I could efficiently help more people each day\n\n2022-2023 Custodian, Honest Cleaning\n\nCleaned a dental office and a plasma donation center both needing great attention to detail to keep patients and employees safe in medical environments\nApplied feedback diligently to improve cleaning efforts to maintain building cleanliness according to the desired expectations of clients and employer\nPlanned hours of work to get all buildings ready for following day’s activities and never received any complaints keeping clients happy\n\n2020-2022 Service Missionary, Austin, Texas\n\nWorked and served 11 hours a day 6 days a week setting up appointments and teaching others\nKept an up to date data source that recorded all contacts that I had with those around me\nHelped manage a Facebook page owned by the mission\n\n2018-2020 Team Lead, Car Wash Express\n\nLed team to complete daily requirements and made sure all tasks were done professionally and correctly\nEntrusted with quickly troubleshooting issues causing cars to not be washed as to maintain customer safety and satisfaction by rebooting systems or clearing the path of the conveyor belt\nPromoted seasonal savings by explaining promotions and encouraging clients to purchase offers and was always a top seller"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Xave Perry ’s Resume",
    "section": "",
    "text": "2017-2020 Idaho State University, Pocatello, ID\n\nA.A in General Studies with a 3.89 GPA\n\nExpected 2025 Brigham Young University - Idaho, Rexburg, ID\n\nBS degree in Computer Science with a 3.80 GPA\nMinor in Computer Engineering"
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Xave Perry ’s Resume",
    "section": "",
    "text": "Languages: Python, C, C#, MySQL, Erlang, Javascript, HTML, CSS, Rust\nSoftware Tools: Visual Studio Code, Tableau, Logism-Evolution, LTspice, STM Cube IDE, Eclipse"
  },
  {
    "objectID": "resume.html#school-projects",
    "href": "resume.html#school-projects",
    "title": "Xave Perry ’s Resume",
    "section": "",
    "text": "Designed a slide puzzle using C and a nucleo-I476 board\nCreated a 4-bit CPU to do basic math and display characters and numbers\nMade an object-oriented banking program in C# using abstraction, encapsulation, inheritance, and polymorphism\nCreated a basic network game that multiple players can connect to a play using Python and TCP connections."
  },
  {
    "objectID": "resume.html#service-and-work-history",
    "href": "resume.html#service-and-work-history",
    "title": "Xave Perry ’s Resume",
    "section": "",
    "text": "2023-2024 Customer Care Representative, Melaleuca\n\nAchieved Star Performer amongst all of the other customer care representatives\nEarned a score within the top 20% of the other agents two months in a row\nChosen to be one of the few agents to take extra calls on the Riverbend Ranch phone line\nImproved my average handle time increasing my volume of phone calls so that I could efficiently help more people each day\n\n2022-2023 Custodian, Honest Cleaning\n\nCleaned a dental office and a plasma donation center both needing great attention to detail to keep patients and employees safe in medical environments\nApplied feedback diligently to improve cleaning efforts to maintain building cleanliness according to the desired expectations of clients and employer\nPlanned hours of work to get all buildings ready for following day’s activities and never received any complaints keeping clients happy\n\n2020-2022 Service Missionary, Austin, Texas\n\nWorked and served 11 hours a day 6 days a week setting up appointments and teaching others\nKept an up to date data source that recorded all contacts that I had with those around me\nHelped manage a Facebook page owned by the mission\n\n2018-2020 Team Lead, Car Wash Express\n\nLed team to complete daily requirements and made sure all tasks were done professionally and correctly\nEntrusted with quickly troubleshooting issues causing cars to not be washed as to maintain customer safety and satisfaction by rebooting systems or clearing the path of the conveyor belt\nPromoted seasonal savings by explaining promotions and encouraging clients to purchase offers and was always a top seller"
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - Project 5",
    "section": "",
    "text": "Did you know that depending on whether someone is a fan of Star Wars or not can be an indicator of the income that someone makes each year? During this project, I was able to clean up data from a survey all about Star Wars and use that data in a AI model to predict whether someone made over $50,000 in a year with about 65% accuracy only looking at how they ranked different characters from the franchise! I was surprised to see just how messy data can be when it is first received even with survey data that you would think would be more organized. As you clean up the data one piece of a time you can make a wonderful database that can make awesome predictions!\n\n\nRead and format project data\n# Include and execute your code here\n# Read the CSV file\n# Read the CSV file with a different encoding\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv\", encoding='latin1')",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - Project 5",
    "section": "",
    "text": "Did you know that depending on whether someone is a fan of Star Wars or not can be an indicator of the income that someone makes each year? During this project, I was able to clean up data from a survey all about Star Wars and use that data in a AI model to predict whether someone made over $50,000 in a year with about 65% accuracy only looking at how they ranked different characters from the franchise! I was surprised to see just how messy data can be when it is first received even with survey data that you would think would be more organized. As you clean up the data one piece of a time you can make a wonderful database that can make awesome predictions!\n\n\nRead and format project data\n# Include and execute your code here\n# Read the CSV file\n# Read the CSV file with a different encoding\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv\", encoding='latin1')",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#fixing-columns-names",
    "href": "Projects/project5.html#fixing-columns-names",
    "title": "Client Report - Project 5",
    "section": "Fixing Columns Names",
    "text": "Fixing Columns Names\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nI was able to fix all of the column names by starting with some of the random columns in the data set, then doing the demographic columns, and then renaming the movie columns and the character columns. I found that with how the .csv was set up their were many columns marked as ‘unnamed’ so I had to find the order that the movies and characters were listed later on in the .csv file to know what each of the unnanmed columns were linked to. I was able to do this with the rename() function in Pandas.\n\n\nRead and format data\n# Include and execute your code here\n\ndf = df.rename(columns={'RespondentID': 'respondent_id',\n\n'Have you seen any of the 6 films in the Star Wars franchise?': 'seen', \n\n'Do you consider yourself to be a fan of the Star Wars film franchise?': 'fan',\n\n'\"Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.\"': 'favorable',\n\n'Which character shot first?': 'shot',\n\n'Are you familiar with the Expanded Universe?': 'exp_uni',\n\n'Do you consider yourself to be a fan of the Expanded Universe?æ': 'fan_exp_uni',\n\n'Do you consider yourself to be a fan of the Star Trek franchise?': 'fan_trek',\n\n'Gender': 'gender',\n'Age': 'age',\n'Household Income': 'income',\n'Education': 'education',\n'Location (Census Region)': 'location',\n\n'Which of the following Star Wars films have you seen? Please select all that apply.': 'ep_1',\n'Unnamed: 4': \"ep_2\",\n'Unnamed: 5': \"ep_3\",\n'Unnamed: 6': \"ep_4\",\n'Unnamed: 7': \"ep_5\",\n'Unnamed: 8': \"ep_6\",\n\n'Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.': 'ep_1_rank',\n'Unnamed: 10': 'ep_2_rank',\n'Unnamed: 11': 'ep_3_rank',\n'Unnamed: 12': 'ep_4_rank',\n'Unnamed: 13': 'ep_5_rank',\n'Unnamed: 14': 'ep_6_rank',\n\n'Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.': 'Han Solo',\n'Unnamed: 16': 'Luke Skywalker',\n'Unnamed: 17': 'Princess Leia Organa',\n'Unnamed: 18': 'Anakin Skywalker',\n'Unnamed: 19': 'Obi Wan Kenobi',\n'Unnamed: 20': 'Emperor Palpatine',\n'Unnamed: 21': 'Darth Vader',\n'Unnamed: 22': 'Lando Calrissian',\n'Unnamed: 23': 'Boba Fett',\n'Unnamed: 24': 'C-3P0',\n'Unnamed: 25': 'R2 D2',\n'Unnamed: 26': 'Jar Jar Binks',\n'Unnamed: 27': 'Padme Amidala',\n'Unnamed: 28': 'Yoda'})\n\n\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Column Names’ execution_count=4}\n\nDisplay column names\n# Include and execute your code here\n\nmydat = df.head(0)\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nrespondent_id\nseen\nfan\nep_1\nep_2\nep_3\nep_4\nep_5\nep_6\nep_1_rank\n...\nYoda\nshot\nexp_uni\nfan_exp_uni\nfan_trek\ngender\nage\nincome\neducation\nlocation\n\n\n\n\n\n\n0 rows × 38 columns\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#clean-and-prepare-data",
    "href": "Projects/project5.html#clean-and-prepare-data",
    "title": "Client Report - Project 5",
    "section": "Clean and Prepare Data",
    "text": "Clean and Prepare Data\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\nI was able to clean the data so that an AI model would be able to make predictions on the data to find whether someone made more than $50,000 in income. I included comments showing where I cleaned up each column listed in the description of this project. I also added an additional clean up for each of the character columns because I wanted to do it a specific way for the model to read it rather than just one-hot encoding it. I found that it was pretty easy to clean up the data using the replace() function that Pandas uses to help find specific strings or data types and replaces it with whatever you specify. One-hot encoding is a powerful tool to quickly format categorical data into data usable for AI models. I also decided to filter the data whether any of the movies were a ‘yes’ on if they had seen it rather than on the general seen column since some of the inputs in the general were marked as ‘no’, but the movies seen were still marked as a ‘yes’.\n\n\nRead and format data\n# Include and execute your code here\n\n# Overall clean up\npd.set_option('display.max_columns', None)\n\ndf.fillna(0, inplace=True)\n\ndf.replace({'Yes': 1, 'No': 0}, inplace=True)\n\n# character clean up\ncharacters_to_modify = ['Han Solo', 'Luke Skywalker', 'Princess Leia Organa', 'Anakin Skywalker', 'Obi Wan Kenobi', 'Emperor Palpatine', 'Darth Vader', 'Lando Calrissian', 'Boba Fett', 'C-3P0', 'R2 D2', 'Jar Jar Binks', 'Padme Amidala', 'Yoda']\n\nfor character in characters_to_modify:\n    df[character] = df[character].replace({'Very favorably': 5, 'Somewhat favorably': 4, 'Neither favorably nor unfavorably (neutral)': 3, 'Somewhat unfavorably': 2, 'Very unfavorably': 1, 'Unfamiliar (N/A)': 0})\n\n#-------------------------------------------------------\n\n#a. Filter the dataset to respondents that have seen at least one film.\nep_columns_to_modify = ['ep_1', 'ep_2', 'ep_3', 'ep_4', 'ep_5', 'ep_6']\n\nfor column in ep_columns_to_modify:\n    df[column] = df[column].replace('[^\\d]+', 1, regex=True).astype(int)\n\ndf = df[(df['seen'] == 1) | (df['seen'] == 0)]\n\n#-------------------------------------------------------\n\n#b. Create a new column that converts the age ranges to a single number. Drop the age range categorical column.\ndf['age'] = df['age'].replace({'18-29': 18, '30-44': 30, '45-60': 45, '&gt; 60': 60}).astype(int)\n\n#-------------------------------------------------------\n\n#c. Create a new column that converts the education groupings to a single number. Drop the school categorical column\ndf['education'] = df['education'].replace({'Graduate degree': 5, 'Bachelor degree': 4, 'Some college or Associate degree': 3, 'High school degree': 2, 'Less than high school degree': 1, 0: 4}).astype(int)\n\n#-------------------------------------------------------\n\n#d. Create a new column that converts the income ranges to a single number. Drop the income range categorical column.\ndf['income'] = df['income'].replace({'$150,000+': 1, '$100,000 - $149,999': 1, '$50,000 - $99,999': 1, '$25,000 - $49,999': 0, '$0 - $24,999': 0, 0: 1}).astype(int)\n\n#-------------------------------------------------------\n\n#e. Create your target (also known as “y” or “label”) column based on the new income range column.\ny = df['income']\n\n#-------------------------------------------------------\n\n#f. One-hot encode all remaining categorical columns.\ncat_cols = ['shot', 'gender', 'location']\n\ndf_encoded = pd.get_dummies(df, columns=cat_cols, dtype=int)\n\ndf = df_encoded",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#validation-of-data",
    "href": "Projects/project5.html#validation-of-data",
    "title": "Client Report - Project 5",
    "section": "Validation of Data",
    "text": "Validation of Data\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article.\nI was able to filter the data down to the same specifications as were used in the article for the most watched films and the best movie of the Star Wars Franchise. As I filtered the data to those specifications I also came up with the total numbers of 835 and 471 respondents for the two different graphs. I made sure to put the movie titles on the y axis and the percentages on the x-axis to match the article charts. I also included images of the article charts so that it is easy to compare my results with their results.\n\n\nRead and format data\n# Include and execute your code here\n\nwith sqlite3.connect('your_database.db') as con:\n    # Write DataFrame to SQLite\n    df.to_sql('star_data', con, index=False, if_exists='replace')\n\n    q_which_films = '''\n    SELECT \n    FLOOR(SUM(ep_1 * 1.0) / COUNT(respondent_id)* 100) AS `The Phantom Menace`,\n    FLOOR(SUM(ep_2 * 1.0) / COUNT(respondent_id)* 100) AS `Attack of the Clones`,\n    CEILING(SUM(ep_3 * 1.0) / COUNT(respondent_id)* 100) AS `Revenge of the Sith`,\n    CEILING(SUM(ep_4 * 1.0) / COUNT(respondent_id)* 100) AS `A New Hope`,\n    CEILING(SUM(ep_5 * 1.0) / COUNT(respondent_id)* 100) AS `The Empire Strikes Back`,\n    FLOOR(SUM(ep_6 * 1.0) / COUNT(respondent_id)* 100) AS `Return of the Jedi`\n    FROM star_data;\n    '''\n    \n    q_best_movie = '''\n\n    SELECT\n    CEILING((SUM(CASE\n            WHEN ep_1_rank = 1\n                THEN 1\n                ELSE 0   \n        END)* 1.0 / 471) * 100) AS `The Phantom Menace`,\n    CEILING((SUM(CASE\n            WHEN ep_2_rank = 1\n                THEN 1\n                ELSE 0   \n        END)* 1.0 / 471) * 100) AS `Attack of the Clones`,\n    CEILING((SUM(CASE\n            WHEN ep_3_rank = 1\n                THEN 1\n                ELSE 0   \n        END)* 1.0 / 471) * 100) AS `Revenge of the Sith`,\n    FLOOR((SUM(CASE\n            WHEN ep_4_rank = 1\n                THEN 1\n                ELSE 0   \n        END)* 1.0 / 471) * 100) AS `A New Hope`,\n    CEILING((SUM(CASE\n            WHEN ep_5_rank = 1\n                THEN 1\n                ELSE 0   \n        END)* 1.0 / 471) * 100) AS `The Empire Strikes Back`,\n    FLOOR((SUM(CASE\n            WHEN ep_6_rank = 1\n                THEN 1\n                ELSE 0   \n        END)* 1.0 / 471) * 100) AS `Return of the Jedi`\n    FROM star_data\n    WHERE \n    ep_1 = 1 AND \n    ep_2 = 1 AND \n    ep_3 = 1 AND \n    ep_4 = 1 AND \n    ep_5 = 1 AND \n    ep_6 = 1;\n    '''\n\n    result_which_films = pd.read_sql_query(q_which_films, con)\n    result_best_movie = pd.read_sql_query(q_best_movie, con)\n\n\nI recreated the article chart inlcuded below with the data after I cleaned it. As you can see the data is still acurate with what the article originally showed.\n\n::: {#cell-Q3 chart A .cell execution_count=7}\n\nRecreating which films chart from article\n# Include and execute your code here\n\n# Reshape the DataFrame into long format\ndf_which_films = pd.melt(result_which_films, value_vars=['Return of the Jedi', 'The Empire Strikes Back', 'A New Hope', 'Revenge of the Sith', 'Attack of the Clones', 'The Phantom Menace'], var_name='Movie', value_name='Percentage')\n\n# Create a bar chart using Plotly Express\nchart = px.bar(df_which_films, \n                x='Percentage', \n                y='Movie', \n                text='Percentage', \n                title=\"Which 'Star Wars' Movies Have You Seen?\")\n\n# Show the chart\nchart.show()\n\n\n                                                \nMost viewed Films\n\n:::\nI recreated the article chart inlcuded below with the data after I cleaned it. As you can see the data is still acurate with what the article originally showed.\n\n::: {#cell-Q3 chart B .cell execution_count=8}\n\nRecreating best movie from article\n# Include and execute your code here\n\n# Reshape the DataFrame into long format\ndf_which_films = pd.melt(result_best_movie, value_vars=['Return of the Jedi', 'The Empire Strikes Back', 'A New Hope', 'Revenge of the Sith', 'Attack of the Clones', 'The Phantom Menace'], var_name='Movie', value_name='Percentage')\n\n# Create a bar chart using Plotly Express\nchart = px.bar(df_which_films, \n                x='Percentage', \n                y='Movie', \n                text='Percentage', \n                title=\"What's the Best 'Star Wars' Movie?\")\n\n# Show the chart\nchart.show()\n\n\n                                                \nBest Movie\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#can-star-wars-predict-income",
    "href": "Projects/project5.html#can-star-wars-predict-income",
    "title": "Client Report - Project 5",
    "section": "Can Star Wars Predict Income?",
    "text": "Can Star Wars Predict Income?\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nI was surprised to see that based off of the favorability of the different Star Wars characters provided in the survey that a model could prediect 60-70% of the time whether someone made over $50,000. It is a large amount of the data, but with how random those rankings are I do not think that I am overloading the model with too much information to skew the results. From the results of this model it seems that if you were to ask someone about their love for these different characters the model could predict their income is over $50k about 65% of the time.\n\n\nRead and format data\n# Include and execute your code here\n\nX = df[['Han Solo', 'Luke Skywalker', 'Princess Leia Organa', 'Anakin Skywalker', 'Obi Wan Kenobi', 'Emperor Palpatine', 'Darth Vader', 'Lando Calrissian', 'Boba Fett', 'C-3P0', 'R2 D2', 'Jar Jar Binks', 'Padme Amidala', 'Yoda']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n\n\n\nCalculate accuracy, conf matrix, class rep\n# Include and execute your code here\n\n# Initialize the Random Forest model\nrf_model = RandomForestClassifier()\n\n# Train the model using the training data\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred_rf = rf_model.predict(X_test)\n\nconf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\nclassification_rep_rf = classification_report(y_test, y_pred_rf)\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\n\nprint(accuracy_rf)\nprint(conf_matrix_rf)\nprint(classification_rep_rf)\n\n\n0.6797752808988764\n[[ 11  90]\n [ 24 231]]\n              precision    recall  f1-score   support\n\n           0       0.31      0.11      0.16       101\n           1       0.72      0.91      0.80       255\n\n    accuracy                           0.68       356\n   macro avg       0.52      0.51      0.48       356\nweighted avg       0.60      0.68      0.62       356",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Client Report - Project 3",
    "section": "",
    "text": "During this project I was able to look into the data covering one of the favorite sports of America, baseball! I was able to look into the salaries of some of the BYUI college players finding one player who made up to 4 million dollars in a single year. I also caclulated batting averages and how they change when the batters have been at the plates more times. Finally, I compared the number of home runs between two of the most documented teams the Cincinnati Reds and the Pittsburgh Pirates and the results may surprise you. Overall, if you are a baseball enthusiast this is the project for you!\n\n\nRead and format project data\n# Include and execute your code here\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#elevator-pitch",
    "href": "Projects/project3.html#elevator-pitch",
    "title": "Client Report - Project 3",
    "section": "",
    "text": "During this project I was able to look into the data covering one of the favorite sports of America, baseball! I was able to look into the salaries of some of the BYUI college players finding one player who made up to 4 million dollars in a single year. I also caclulated batting averages and how they change when the batters have been at the plates more times. Finally, I compared the number of home runs between two of the most documented teams the Cincinnati Reds and the Pittsburgh Pirates and the results may surprise you. Overall, if you are a baseball enthusiast this is the project for you!\n\n\nRead and format project data\n# Include and execute your code here\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#byui-college-player-salaries",
    "href": "Projects/project3.html#byui-college-player-salaries",
    "title": "Client Report - Project 3",
    "section": "BYUI College Player Salaries",
    "text": "BYUI College Player Salaries\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nI found that Lindsma01 had the highset salary for many of the later years, and Stephga01 had the highest salary in some of the ealier years. The highest salary was in 2014 for $4,000,000.\nI found that there was a wide range of salaries among the BYUI players and that there were only 2 of them in this data set.\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Salary of BYUI Players’ execution_count=3}\n\nSalary of BYUI players\n# Include and execute your code here\n\nq = '''\nSELECT DISTINCT c.playerID, c.schoolID, s.salary, s.yearID, s.teamID \nFROM collegeplaying c JOIN \nsalaries s  ON c.playerID = s.playerID \nWHERE schoolID = \"idbyuid\" \nORDER BY s.salary desc\n'''\nresults = pd.read_sql_query(q,con)\n\nresults\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nsalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n4000000.0\n2014\nCHA\n\n\n1\nlindsma01\nidbyuid\n3600000.0\n2012\nBAL\n\n\n2\nlindsma01\nidbyuid\n2800000.0\n2011\nCOL\n\n\n3\nlindsma01\nidbyuid\n2300000.0\n2013\nCHA\n\n\n4\nlindsma01\nidbyuid\n1625000.0\n2010\nHOU\n\n\n5\nstephga01\nidbyuid\n1025000.0\n2001\nSLN\n\n\n6\nstephga01\nidbyuid\n900000.0\n2002\nSLN\n\n\n7\nstephga01\nidbyuid\n800000.0\n2003\nSLN\n\n\n8\nstephga01\nidbyuid\n550000.0\n2000\nSLN\n\n\n9\nlindsma01\nidbyuid\n410000.0\n2009\nFLO\n\n\n10\nlindsma01\nidbyuid\n395000.0\n2008\nFLO\n\n\n11\nlindsma01\nidbyuid\n380000.0\n2007\nFLO\n\n\n12\nstephga01\nidbyuid\n215000.0\n1999\nSLN\n\n\n13\nstephga01\nidbyuid\n185000.0\n1998\nPHI\n\n\n14\nstephga01\nidbyuid\n150000.0\n1997\nPHI\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#batting-average",
    "href": "Projects/project3.html#batting-average",
    "title": "Client Report - Project 3",
    "section": "Batting Average",
    "text": "Batting Average\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report. Use the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results. Now calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\nEvery table has unique players that were shown in the top 5 best average scores. I also noticed that as the number of AB’s was increased the average number went down in value with the highest score after at least 100 bats was 40% of those being hits. Hazlebo01 was the best hitter of those who had at least 100 bats.\nAll of the tables have unique player_ID’s which I thought was intersting showing that with even a 10 AB increase there was a whole new set of players in the top 5. As the AB’s were increased the averages got lower and lower.\n::: {#cell-Q2 table ‘a’ .cell .tbl-cap-location-top tbl-cap=‘Batting Average for 0 or More Hits’ execution_count=4}\n\nBatting average for 0 or more hits\n# Include and execute your code here\n\nq = '''\nSELECT playerID, yearID, (H * 1.0 / AB) AS average\nFROM batting\nWHERE AB &gt; 0\nORDER BY average desc, playerID\nLIMIT 5\n'''\nresults = pd.read_sql_query(q,con)\n\nresults\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\naverage\n\n\n\n\n0\naberal01\n1957\n1.0\n\n\n1\nabernte02\n1960\n1.0\n\n\n2\nabramge01\n1923\n1.0\n\n\n3\nacklefr01\n1964\n1.0\n\n\n4\nalanirj01\n2019\n1.0\n\n\n\n\n\n\n:::\n::: {#cell-Q2 table ‘b’ .cell .tbl-cap-location-top tbl-cap=‘Batting Average for 10 Hits or More’ execution_count=5}\n\nBatting average for 10 hits or more\n# Include and execute your code here\n\nq = '''\nSELECT playerID, yearID, (H * 1.0 / AB) AS average\nFROM batting\nWHERE AB &gt;= 10\nORDER BY average desc, playerID\nLIMIT 5\n'''\nresults = pd.read_sql_query(q,con)\n\nresults\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\naverage\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\njohnsde01\n1975\n0.600000\n\n\n4\nsilvech01\n1948\n0.571429\n\n\n\n\n\n\n:::\n::: {#cell-Q2 table ‘c’ .cell .tbl-cap-location-top tbl-cap=‘Top 5 Total All Time Batting Averages’ execution_count=6}\n\nTotal hits\n# Include and execute your code here\n\nq = '''\nSELECT playerID, SUM(H * 1.0) / NULLIF(SUM(AB), 0) AS average_total\nFROM batting\nWHERE AB &gt;= 100\nGROUP BY playerID\nORDER BY average_total desc, playerID\nLIMIT 5\n'''\nresults = pd.read_sql_query(q,con)\n\nresults\n\n\n\n\n\n\n\n\n\nplayerID\naverage_total\n\n\n\n\n0\nhazlebo01\n0.402985\n\n\n1\ndaviscu01\n0.380952\n\n\n2\nfishesh01\n0.374016\n\n\n3\nwoltery01\n0.369565\n\n\n4\ncobbty01\n0.366299\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#reds-or-pirates",
    "href": "Projects/project3.html#reds-or-pirates",
    "title": "Client Report - Project 3",
    "section": "Reds or Pirates?",
    "text": "Reds or Pirates?\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nWhen choosing which two teams to compare, I first used a SQL query to find which two teams had the most data and the Reds and Pirates were two of the largest outliers with 129 occurences each. My results were that the Reds had far more home runs than the Pirates meaning that the Reds must have had much better hitters than the Pirates did.\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Home runs between Cincinnati and Pittsburgh’ execution_count=7}\n\nmaking table to show HR throughout the years\n# Include and execute your code here\n\nq = '''\nSELECT name team_name, sum(HR) home_runs, yearID year\nFROM teams\nWHERE name = \"Pittsburgh Pirates\" OR name = \"Cincinnati Reds\" AND yearID &gt;= 1891\nGROUP BY yearID, name\n'''\nresults = pd.read_sql_query(q,con)\n\nresults\n\n\n\n\n\n\n\n\n\nteam_name\nhome_runs\nyear\n\n\n\n\n0\nCincinnati Reds\n40\n1891\n\n\n1\nPittsburgh Pirates\n29\n1891\n\n\n2\nCincinnati Reds\n44\n1892\n\n\n3\nPittsburgh Pirates\n38\n1892\n\n\n4\nCincinnati Reds\n29\n1893\n\n\n...\n...\n...\n...\n\n\n247\nPittsburgh Pirates\n151\n2017\n\n\n248\nCincinnati Reds\n172\n2018\n\n\n249\nPittsburgh Pirates\n157\n2018\n\n\n250\nCincinnati Reds\n227\n2019\n\n\n251\nPittsburgh Pirates\n163\n2019\n\n\n\n\n252 rows × 3 columns\n\n\n:::\nHere is the basic comparison of the two teams yearly home runs from 1891-2019. It is easy to see the the blue line representing Cincinnati is almost always above the other teams number of home runs. From this chart, I can tell that the Cincinnati team must have had much better hitters than the Pittsburgh team.\n::: {#cell-Q3 chart .cell execution_count=8}\n\nComparing HR\n# Include and execute your code here\n\nteam_colors = {\n    \"Pittsburgh Pirates\": \"blue\",\n    \"Cincinnati Reds\": \"red\"\n}\n\nchart = px.line(results,\n    x=\"year\", \n    y=\"home_runs\",\n    color=\"team_name\",\n    width=900,\n    color_discrete_map=team_colors,\n    title=\"Homes Runs between Cincinnati and Pittsburgh\"\n    \n)\n\nchart.update_xaxes(tickvals=[year for year in range(results['year'].min(), results['year'].max()+1, 16)])\n\nchart.update_yaxes(tickvals=[val for val in range(0, results['home_runs'].max()+1, 50)])\n\nchart.show()\n\n\n                                                \nComparing HR\n\n:::\nI included a second chart of the same data with the line chart greyed out to show the difference in the trendlines of both graphs. It is easy to see that the Cincinnati team was above the other team for most of the years according to the trendline.\n::: {#cell-Q3 chart with trendline .cell execution_count=9}\n\nTrendline comparison\n# Include and execute your code here\n\nteam_colors = {\n    \"Pittsburgh Pirates\": \"silver\",\n    \"Cincinnati Reds\": \"grey\"\n}\n\nchart = px.line(results,\n    x=\"year\", \n    y=\"home_runs\",\n    color=\"team_name\",\n    width=900,\n    color_discrete_map=team_colors,\n    title=\"Homes Runs between Cincinnati and Pittsburgh\"\n)\n\n# calculate trendlines by finding slope and y intercept\nteam_count = 1\nfor team_name in results['team_name'].unique():\n    if team_count == 1:\n      team_data = results[results['team_name'] == team_name]\n      m, b = np.polyfit(team_data['year'], team_data['home_runs'], 1)\n      trendline_y = m * results['year'] + b\n      chart.add_scatter(x=results['year'], y=trendline_y, mode='lines', line=dict(color='red', dash='dash'), name=f'Trendline - {team_name}')\n    else:\n      team_data = results[results['team_name'] ==   team_name]\n      m, b = np.polyfit(team_data['year'], team_data['home_runs'], 1)\n      trendline_y = m * results['year'] + b\n      chart.add_scatter(x=results['year'], y=trendline_y, mode='lines', line=dict(color='blue', dash='dash'), name=f'Trendline - {team_name}')\n    team_count += 1\n\nchart.update_xaxes(tickvals=[year for year in range(results['year'].min(), results['year'].max()+1, 16)])\n\nchart.update_yaxes(tickvals=[val for val in range(0, results['home_runs'].max()+1, 50)])\n\nchart.show()\n\n\n                                                \nTrendline Comparison\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Client Report - Project 1",
    "section": "",
    "text": "I was able to find the uses of my name throughout history and compare it to the year that I was born, and the data is customizable to have it show the data of most names. I found that if you were to get a call from someone named Brittany you could guess her age to be around 30-40 years old. The names Mary, Martha, Peter, and Paul were common Christian names and I found that Mary was by far the most common name from 1920-2000. Finally I found strong evidence to support that the movie Titanic inpsired many people to name their kids Rose. There is a lot that we can learn from a name and this project shows a small amount of information we can create soley from data about names.\n\n\nRead and format project data\n# Include and execute your code here\n\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv\"\ndf = pd.read_csv(url)",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#elevator-pitch",
    "href": "Projects/project1.html#elevator-pitch",
    "title": "Client Report - Project 1",
    "section": "",
    "text": "I was able to find the uses of my name throughout history and compare it to the year that I was born, and the data is customizable to have it show the data of most names. I found that if you were to get a call from someone named Brittany you could guess her age to be around 30-40 years old. The names Mary, Martha, Peter, and Paul were common Christian names and I found that Mary was by far the most common name from 1920-2000. Finally I found strong evidence to support that the movie Titanic inpsired many people to name their kids Rose. There is a lot that we can learn from a name and this project shows a small amount of information we can create soley from data about names.\n\n\nRead and format project data\n# Include and execute your code here\n\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv\"\ndf = pd.read_csv(url)",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#xavier-throughout-history",
    "href": "Projects/project1.html#xavier-throughout-history",
    "title": "Client Report - Project 1",
    "section": "Xavier throughout History",
    "text": "Xavier throughout History\nHow does your name at your birth year compare to its use historically?\nI found that the name Xavier was used very rarely in the early 1900’s and didn’t even pass 100 uses until the year 1966! When I was born in 2001 there was a big increase in the uses of the name Xavier and it continued to increase until 2007 capping at 6552 uses. Although there is a decline beginning to happen I found that Xavier has definitely increased in popularity over time.\n\n\nRead and format data\n# Include and execute your code here\n\n#Filter names to only Xavier\ndf_xavier = df[df['name'] == 'Xavier']\n\n\nHere is a line chart showing the data that has the name Xavier. I put an anotation on the year 2001 so that it is easy to see which is the year of focus for this data set.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nDesigned line graph, added anotations for my birth year\n# Include and execute your code here\n\n# Create Line Chart\nchart = px.line(df_xavier,  \nx='year',\ny='Total',\ncolor = None,\nlabels= {'Total': 'Number of Uses', 'year': 'Year'},\ntitle = \"Xavier throughout History\")\n\n#Declare tick mark settings\nchart.update_xaxes(tickmode='linear', tick0=df_xavier['year'].min(), dtick=1)\n\nchart.show()\n\n\n                                                \nUses of the name Xavier\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Table of the uses of Xavier’ execution_count=5}\n\ntable of the uses of Xavier\n# Include and execute your code here\n\n#Limit the years shown on the table to 1996-2005\ndf_xavier_filtered = df_xavier[(df_xavier['year'] &gt;= 1996) & (df_xavier['year'] &lt;= 2005)]\n\nmydat = df_xavier_filtered.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(15)\\\n    .filter(['year', 'Total'])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nTotal\n\n\n\n\n0\n1996\n2484.0\n\n\n1\n1997\n2575.0\n\n\n2\n1998\n2906.0\n\n\n3\n1999\n3164.0\n\n\n4\n2000\n3627.0\n\n\n5\n2001\n3974.0\n\n\n6\n2002\n4647.0\n\n\n7\n2003\n5060.0\n\n\n8\n2004\n4997.5\n\n\n9\n2005\n4873.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#can-you-guess-the-age",
    "href": "Projects/project1.html#can-you-guess-the-age",
    "title": "Client Report - Project 1",
    "section": "Can you guess the age?",
    "text": "Can you guess the age?\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nI found that there was a dramatic increase in the uses of the name Brittany starting in 1980 and ending in 1990 which was followed by an almost equally dramatic decline in the use of the name Brittany. From the data I conculded that if you were to receive a phone call from someone named Brittany then you could make a guess that she was born sometime between the years 1986-1994 with the greatest amount of Brittany’s being born in 1990. That would give an age range of 30-38 years old. I would not guess the ages of 46 and older or 14 and younger.\n\n\nRead and format data\n# Include and execute your code here\n\n#Filter names to only Brittany\ndf_brittany = df[df['name'] == 'Brittany']\n\n\nHere is a line chart showing the data of those with the name of Brittany. I included an annotation sharing the year with the highest uses of the name brittany, the actual value of that year, and the age that someone would be if they were born in that year.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nDesign line graph, add an annotation showing highest used year and calculated age.\n# Include and execute your code here\n\n#Create Line Chart\nchart = px.line(df_brittany,  \nx='year',\ny='Total',\ncolor = None,\nlabels= {'Total': 'Number of Uses', 'year': 'Year'},\ntitle = \"How old is Brittany?\")\n\n#Declare tick mark settings\nchart.update_xaxes(tickmode='linear', tick0=df_brittany['year'].min(), dtick=20)\n\n#Add an annotation showing the peak of the use of the name Brittany\nhighlight_year = 1990\nage_highlight_year = 2024-1990\nhighlight_value = df_brittany.loc[df_brittany['year'] == highlight_year, 'Total'].iloc[0]\nchart.add_annotation(\n    x=highlight_year,\n    y=highlight_value,\n    text=f'Year: {highlight_year} Value: {highlight_value} Age: {age_highlight_year}',\n    showarrow=True,\n    arrowhead=2,\n    arrowsize=1,\n    arrowwidth=2,\n    arrowcolor='black',\n    font=dict(size=12, color='black',),\n    bgcolor='yellow',\n    opacity=1\n)\n\nchart.show()\n\n\n                                                \nHow old is Brittany?\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Popular Decade for the Name of Brittany’ execution_count=8}\n\ntable showing brittany data\n# Include and execute your code here\n\n#Limit to only the years 1985-1995\ndf_brittany_filtered = df_brittany[(df_brittany['year'] &gt;= 1985) & (df_brittany['year'] &lt;= 1995)]\n\nmydat = df_brittany_filtered.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter(['year', 'name', 'Total'])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nname\nTotal\n\n\n\n\n1\n1986\nBrittany\n17856.5\n\n\n2\n1987\nBrittany\n18825.5\n\n\n3\n1988\nBrittany\n21952.0\n\n\n4\n1989\nBrittany\n30848.0\n\n\n5\n1990\nBrittany\n32562.5\n\n\n6\n1991\nBrittany\n26963.5\n\n\n7\n1992\nBrittany\n23416.5\n\n\n8\n1993\nBrittany\n21728.0\n\n\n9\n1994\nBrittany\n17808.5\n\n\n10\n1995\nBrittany\n15875.5\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#popularity-contest",
    "href": "Projects/project1.html#popularity-contest",
    "title": "Client Report - Project 1",
    "section": "Popularity Contest",
    "text": "Popularity Contest\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\nI found that of the names Mary, Martha, Peter, and Paul that Mary was by far the most commonly used name by somtimes double the amount of the other names. I also found a common trend that was interesting. Around the year 1956 all 4 names seemed to all start declining in uses and never reached the peaks they had in the past. At the end of the data it can also be seen that they all seem to be becoming about the same popularity level with Mary still slightly in the lead.\n\n\nRead and format data\n# Include and execute your code here\n\n#Filter names to Mary, Martha, Peter, and Paul and from the range 1920-2000\nnames_to_filter = ['Mary', 'Martha', 'Peter', 'Paul']\nyears_to_filter = [year for year in range(1910, 1920)] + [year for year in range(2001, 2016)]\n\ndf_christian_names = df[df['name'].isin(names_to_filter) & ~df['year'].isin(years_to_filter)]\n\n\nHere is a line chart that shows the uses of 4 common Christian names from the years 1920-2000. I added an annotation and a dashed line to draw attention to a common theme of decline among all 4 names.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nDesigned a line chart, added a dashed line through 1956, and added an annotation\n# Include and execute your code here\n\n#Create Line Graph\nchart = px.line(df_christian_names,  \nx='year',\ny='Total',\ncolor = 'name',\nlabels= {'Total': 'Number of Uses', 'year': 'Year'},\ntitle = \"Popular Christian Names Through Time\")\n\n#Declare tick mark settings\nchart.update_xaxes(tickmode='linear', tick0=df_christian_names['year'].min(), dtick=20)\n\n#Add dashed line to year 1956\nchart.add_shape(\n    go.layout.Shape(\n        type='line',\n        x0=1956,\n        x1=1956,\n        y0=df_christian_names['Total'].min(),\n        y1=df_christian_names['Total'].max(),\n        line=dict(color='black', width=2, dash='dash'),\n        opacity=0.5\n    )\n)\n\n#Add annotation to describe dashed line's purpose of showing the decline\nchart.add_annotation(\n    x=1956,\n    y=df_christian_names['Total'].max(),\n    text='Decline Begins',\n    showarrow=True,\n    arrowhead=4,\n    ax=0,\n    ay=-40\n)\n\nchart.show()\n\n\n                                                \nPopular Christian names through time\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Table of Popular Christian names’ execution_count=11}\n\ntable for popular baby names\n# Include and execute your code here\n\nmydat = df_christian_names.head(1000)\\\n    .groupby('name')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter(['name', 'Total'])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nname\nTotal\n\n\n\n\n0\nMartha\n417231.5\n\n\n1\nMary\n2213033.5\n\n\n2\nPaul\n1090579.5\n\n\n3\nPeter\n466809.5\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#titanic-inspiration",
    "href": "Projects/project1.html#titanic-inspiration",
    "title": "Client Report - Project 1",
    "section": "Titanic Inspiration",
    "text": "Titanic Inspiration\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nThe name that I thought of was Rose from the movie Titanic. The year that Titanic was released was 1997 and the data shows on the line graph that immediatly following that year there was a spike of the use of Rose by almost 400 uses per year for the following 4 years. I would have to conclude that there was an effect after the release of Titanic on the uses of the name Rose. I also found it interesting that the name Rose has spiked tremendously in the last few years doubling the previously highest uses.\n\n\nRead and format data\n# Include and execute your code here\n\n#Filter to names of Rose used from the years 1990-2015\nyears_to_filter = [year for year in range(1990, 2016)]\n\ndf_rose = df[df['name'].isin(['Rose']) & df['year'].isin(years_to_filter)]\n\n\nI created a line chart to display the data of the use of the name Rose. I added a dashed line to show the release year of the movie Titanic. It is easy to see a dramatic increase immediately after the release of the movie.\n::: {#cell-Q4 chart .cell execution_count=13}\n\nCreation of line graph, line at 1997, and an annotation\n# Include and execute your code here\n\n#Create Line Graph\nchart = px.line(df_rose,  \nx='year',\ny='Total',\ncolor = None,\nlabels= {'Total': 'Number of Uses', 'year': 'Year'},\ntitle = 'How &lt;i&gt;Titanic&lt;/i&gt; impacted the use of Rose')\n\n#Declare tick mark settings\nchart.update_xaxes(tickmode='linear', tick0=df_rose['year'].min(), dtick=5)\n\n#Add dashed line on year 1997 for movie release\nchart.add_shape(\n    go.layout.Shape(\n        type='line',\n        x0=1997,\n        x1=1997,\n        y0=df_rose['Total'].min(),\n        y1=df_rose['Total'].max(),\n        line=dict(color='black', width=2, dash='dash')\n    )\n)\n\n#Describes dashed line's purpose\nchart.add_annotation(\n    x=1997,\n    y=df_rose['Total'].max(),\n    text=f'1997 - &lt;i&gt;Titanic&lt;/i&gt;',\n    showarrow=True,\n    arrowhead=4,\n    ax=0,\n    ay=-40\n)\n\nchart.show()\n\n\n                                                \nHow Titanic impacted the use of Rose\n\n:::\n::: {#cell-Q4 table .cell .tbl-cap-location-top tbl-cap=‘Table of uses of Rose from 1995-2015’ execution_count=14}\n\ntable for uses of Rose\n# Include and execute your code here\n\nmydat = df_rose.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(21)\\\n    .filter(['year', 'name', 'Total'])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nname\nTotal\n\n\n\n\n5\n1995\nRose\n742.0\n\n\n6\n1996\nRose\n673.0\n\n\n7\n1997\nRose\n621.0\n\n\n8\n1998\nRose\n998.0\n\n\n9\n1999\nRose\n1093.0\n\n\n10\n2000\nRose\n1056.0\n\n\n11\n2001\nRose\n1019.0\n\n\n12\n2002\nRose\n907.0\n\n\n13\n2003\nRose\n816.0\n\n\n14\n2004\nRose\n862.0\n\n\n15\n2005\nRose\n944.0\n\n\n16\n2006\nRose\n933.0\n\n\n17\n2007\nRose\n914.0\n\n\n18\n2008\nRose\n937.0\n\n\n19\n2009\nRose\n904.0\n\n\n20\n2010\nRose\n920.0\n\n\n21\n2011\nRose\n1073.0\n\n\n22\n2012\nRose\n1225.0\n\n\n23\n2013\nRose\n1408.0\n\n\n24\n2014\nRose\n1666.0\n\n\n25\n2015\nRose\n1916.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Xave Perry\n ## A little about me\nI am from Idaho Falls, Idaho and have been here most of my life. I have a wonderful wife and an amazing dog that make me very happy. I have a passion for coding and I have really enjoyed my time at BYU-I studying Computer Science.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Client Report - Project 2",
    "section": "",
    "text": "This project was full of awesome insights and data about flights from airports across the United States. I was able to find the proportion of delays to the average amount of hours that each of those delays took to find the airport that when a delay occurs everyone should get comfy. I also looked deeply into weather delays and how it effects each of the airports. I also found which month is the best month to fly in with the least amount of delays. If you want to be a flying expert, I would definitely recommend looking at this data and notice how tidy it all is while you’re at it.\n\n\nRead and format project data\n# Include and execute your code here\n\nwith open('json/flights.json', 'r') as file:\n    data = json.load(file)\n\ndf = pd.DataFrame(data)",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#elevator-pitch",
    "href": "Projects/project2.html#elevator-pitch",
    "title": "Client Report - Project 2",
    "section": "",
    "text": "This project was full of awesome insights and data about flights from airports across the United States. I was able to find the proportion of delays to the average amount of hours that each of those delays took to find the airport that when a delay occurs everyone should get comfy. I also looked deeply into weather delays and how it effects each of the airports. I also found which month is the best month to fly in with the least amount of delays. If you want to be a flying expert, I would definitely recommend looking at this data and notice how tidy it all is while you’re at it.\n\n\nRead and format project data\n# Include and execute your code here\n\nwith open('json/flights.json', 'r') as file:\n    data = json.load(file)\n\ndf = pd.DataFrame(data)",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#tidying-data",
    "href": "Projects/project2.html#tidying-data",
    "title": "Client Report - Project 2",
    "section": "Tidying Data",
    "text": "Tidying Data\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”).\nI was able to use the replace library tool to find any data that was blank or equal to ‘null’ and changed it to ‘NaN’ so that there was no empty data. I displayed this by showing the airport names and how some of them were left blank so the name for that airport now is displayed with NaN in it’s place. There were lots of data that were left blank along with the airport name, like the month column for example.\n\n\nRead and format data\n# Include and execute your code here\n\n# Replace empty strings and \"null\" with 'NaN' in the 'airport_name' column\ndf.replace(['', 'null'], 'NaN', inplace=True)\n\n# Replace incorrect spelling of February.\ndf.replace(['Febuary'], 'February', inplace=True)\n\n\nThis table was made to display the airport name so that the NaN value could be seen. I added additional data points to show more detail about numbers for each of the airport’s and how NaN still has values as well.\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Tidy data’ execution_count=4}\n\nsummed the total number of flights and delays and showed them on the table along with the airport names. Main goal was to show the NaN value.\n# Include and execute your code here\n\n# Define columns and their aggregation functions\nagg_columns = {\n    'num_of_flights_total': 'sum',\n    'num_of_delays_total': 'sum',\n}\n\n# Group by 'airport_name' and apply the aggregation functions\ncombined_df = df.groupby('airport_name').agg(agg_columns).reset_index()\n\n# Display the result\ndisplay(combined_df)\n\n\n\n\n\n\n\n\n\nairport_name\nnum_of_flights_total\nnum_of_delays_total\n\n\n\n\n0\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\n4235114\n870910\n\n\n1\nChicago, IL: Chicago O'Hare International\n3400032\n773122\n\n\n2\nDenver, CO: Denver International\n2323376\n439964\n\n\n3\nNaN\n884879\n172413\n\n\n4\nSalt Lake City, UT: Salt Lake City International\n1293072\n190733\n\n\n5\nSan Diego, CA: San Diego International\n870161\n167747\n\n\n6\nSan Francisco, CA: San Francisco International\n1565257\n408631\n\n\n7\nWashington, DC: Washington Dulles International\n773480\n152630\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#airports-to-avoid",
    "href": "Projects/project2.html#airports-to-avoid",
    "title": "Client Report - Project 2",
    "section": "Airports to Avoid",
    "text": "Airports to Avoid\nWhich airport has the worst delays?\nTo find the airport with the worst delays I created a table that shows the airport code, the total number of flights, and the total number of delays. I also calculated the total delays per average hours of delay to show how long each delay was each time it happened. Because the data was originally in minutes I changed the minutes delayed total to hours by dividing it by 60. To my surpirse, I found that the San Francisco airport even with it’s low numbers of overall flights had the worst wait time ratio with 4.66%. That was closely followed by Salt Lake City, and Denver. I would say that the airport with the worst delays would be San Francisco, but the pool of data was lower for that variable so there could be some fault due to lack of data.\n\n\nRead and format data\n# Include and execute your code here\n\nagg_columns = {\n    'num_of_flights_total': 'sum',\n    'num_of_delays_total': 'sum',\n    'minutes_delayed_total': lambda x: x.mean(),  # find average\n}\n\n# Group by 'airport_name' and apply the aggregation functions\ncombined_df = df.groupby('airport_code').agg(agg_columns).reset_index()\n\n# Calculate the percentage of delays in relation to total flights in hours\ncombined_df['ttl_delays/hrs_delay'] = (combined_df['num_of_delays_total'] /(combined_df['minutes_delayed_total'])/60) * 100 # convert minutes to hours in calculation\n\n# Format integer columns with commas for better readability\nfor col in ['num_of_flights_total', 'num_of_delays_total']:\n    combined_df[col] = combined_df[col].apply(lambda x: '{:,.0f}'.format(x))\n\n# Format the percentage column\ncombined_df['ttl_delays/avg_hrs_delay'] = combined_df['ttl_delays/hrs_delay'].apply(lambda x: '{:.2f}%'.format(x))\n\n\nThis table was made to show the metrics that I chose to make my point of which airport was the worst with delays. The final column was the proportion between the total number of delays and the number of hours that each delay had to show trends that would not be seen if you only compared the total number of delays alone.\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Airports to Avoid’ execution_count=6}\n\nshow only the desired columns in a table\n# Include and execute your code here\n\nmydat = combined_df\\\n    .reset_index()\\\n    .filter(['airport_code', 'num_of_flights_total', 'num_of_delays_total', 'ttl_delays/avg_hrs_delay'])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_flights_total\nnum_of_delays_total\nttl_delays/avg_hrs_delay\n\n\n\n\n0\nATL\n4,430,047\n902,443\n3.68%\n\n\n1\nDEN\n2,513,974\n468,519\n4.09%\n\n\n2\nIAD\n851,571\n168,467\n3.60%\n\n\n3\nORD\n3,597,588\n830,825\n3.24%\n\n\n4\nSAN\n917,862\n175,132\n4.66%\n\n\n5\nSFO\n1,630,945\n425,604\n3.53%\n\n\n6\nSLC\n1,403,384\n205,160\n4.46%\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#best-month-to-fly",
    "href": "Projects/project2.html#best-month-to-fly",
    "title": "Client Report - Project 2",
    "section": "Best Month to Fly",
    "text": "Best Month to Fly\nWhat is the best month to fly if you want to avoid delays of any length?\nAll of the different months it seem to be pretty consistent along with some low outliers of September and November, and some high outliers of June, July, and December. November is the leading month for the lowest number of Delays by about 4,000. So from the data I would say that the best month to fly in is November with September as a close second.\n\n\nRead and format data\n# Include and execute your code here\n\ndf_filtered_month = df[df['month'] != 'n/a']\n\n\nIn the bar graph I excluded any months that were null so that only the labeled months were shown. I used the total number of delays as the metric of measurement. I added a dashed line at the height of the November bar so that the value could easily be compared with the rest of the months. I also noticed that February was mispelt in the data so I changed every value of “Febuary” to the correct spelling in the first step when I replaced all null values with NaN.\n::: {#cell-Q3 chart .cell execution_count=8}\n\ntotaled number of delays and grouped by month, placed the months in the correct order, displayed in a bar chart and added a line at the minimun delay value.\n# Include and execute your code here\n\n# group and filter\ndf_month_sum = df_filtered_month.groupby(\"month\", as_index=False)[\"num_of_delays_total\"].sum()\n\n# order the months correctly\nmonth_order = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n\n\nchart = px.bar(df_month_sum,\n               x=\"month\",\n               y=\"num_of_delays_total\",\n               color=\"month\",\n               category_orders={'month': month_order},\n               title='Best Month to Fly'\n)\n\n# add line at the lowest delay value\nlowest_value = df_month_sum[\"num_of_delays_total\"].min()\nchart.add_shape(type=\"line\",\n               x0=df_month_sum[\"month\"].iloc[4],\n               x1=df_month_sum[\"month\"].iloc[2],\n               y0=lowest_value,\n               y1=lowest_value,\n               line=dict(color=\"black\", width=2, dash='dash')\n)\n\nchart.show()\n\n\n                                                \nBest Month to Fly\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#weather-delays",
    "href": "Projects/project2.html#weather-delays",
    "title": "Client Report - Project 2",
    "section": "Weather Delays",
    "text": "Weather Delays\nYour job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). Using the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\nI was able to calculate the total weather for each of the different rows of data. Using the rules that were outlined in the project I calculated the results and put it in a column called “total_weather”. I also added an additional column called “total_before_calc” that did not apply any of the rules and just added all three columns as they were without taking off any values as a comparison for how the rules effected the outcome of the data.\n\n\nRead and format data\n# Include and execute your code here\n\n# Exlcude NaN values\ndf_filtered_negative_values = df[df[\"num_of_delays_late_aircraft\"] != -999]\n\n# Calculate the average excluding NaN values\naverage_delay = df_filtered_negative_values[\"num_of_delays_late_aircraft\"].mean()\n\n# Replace -999 with the calculated average in the original DataFrame\ndf[\"num_of_delays_late_aircraft\"].replace(-999, average_delay, inplace=True)\n\n# 100% of weather delays and 30% of late delays.\ndf['total_weather'] = np.ceil(df['num_of_delays_weather'] + df['num_of_delays_late_aircraft'] * 0.3)\n\n# Check if the month is in the specified list\nis_summer_month = df['month'].isin(['April', 'May', 'June', 'July', 'August'])\n\n# 40% or 60% depending on month\n# Apply different multipliers based on the month\ndf['total_weather'] += np.where(is_summer_month, np.ceil(df['num_of_delays_nas'] * 0.4), np.ceil(df['num_of_delays_nas'] * 0.65))\n\n# Round 'total_weather' to the nearest integer\ndf['total_weather'] = df['total_weather'].astype(int)\n\n# 100% of each column to show the difference in the table.\ndf['total_before_calc'] = df['num_of_delays_weather'] + df['num_of_delays_late_aircraft'] + df['num_of_delays_nas']\n\ndf['total_before_calc'] = df['total_before_calc'].astype(int)\n\n\nMade a table with the first 5 rows of data. I replaced each of the values of “-999” which I assumed to be null with the average value of 1109.143. I while calculating the totals I also used the np.ceil function to round the numbers to the next integer since it does not make sense to have a portion of a weather delay.\n::: {#cell-Q4 table .cell .tbl-cap-location-top tbl-cap=‘Weather Delays’ execution_count=10}\n\nshowing only the first 5 rows of data, using custom columns defined above.\n# Include and execute your code here\n\nmydat = df.head(5)\\\n    .reset_index()\\\n    .filter([\"total_weather\", \"total_before_calc\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\ntotal_weather\ntotal_before_calc\n\n\n\n\n0\n3770\n6155\n\n\n1\n1120\n2096\n\n\n2\n961\n2014\n\n\n3\n4503\n7976\n\n\n4\n675\n1374\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#weather-delays-at-each-airport",
    "href": "Projects/project2.html#weather-delays-at-each-airport",
    "title": "Client Report - Project 2",
    "section": "Weather Delays at each Airport",
    "text": "Weather Delays at each Airport\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\nI found that the total weather delays of all the different airports vary greatly. There are three noticable groups of data ranges. ATL and ORD are in the 300,000 range, DEN, and SFO are in the 150,000 range, and IAD, SAN, and SLC are in the 50,000 range. Thinking about each of this airports geographically I am not sure why they are grouped this way. ATL and ORD are more eastern which is somewhat of a similiarity but still not very satisfying since DC is also to the East. Regardless of geographical location, you are much more likely to have delays due to weather in ATL and ORD airports and much less likely in IAD, SAN, and SLC.\n\n\nRead and format data\n# Include and execute your code here\n\ndf_weather = df\n\n\nI summed of all of the total weather numbers and grouped them by the airport code to easily see the total values of each of the bars. I also chose to use the codes for easier viewing since the names of the airports are quite long.\n::: {#cell-Q5 chart .cell execution_count=12}\n\ngrouped and summed data, showed in form of a bar chart.\n# Include and execute your code here\n\n\n# group by code and total weather sum\ndf_weather_sum = df_weather.groupby(\"airport_code\", as_index=False)[\"total_weather\"].sum()\n\nchart = px.bar(df_weather_sum.head(200),\n               x=\"airport_code\",\n               y=\"total_weather\",\n               color=\"airport_code\",\n               title=\"Weather Delays at each Airport\"\n)\n\nchart.show()\n\n\n                                                \nWeather Delays at each Airport\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "Client Report - Project 4",
    "section": "",
    "text": "Using AI, I was able to find specific features of homes that can accuratly predict whether a home was built before or after 1980. Some of those things were the number of bathrooms, the total number of living space in the home, and the architecture style of the home. You would be surprised to see how many homes that are rated as great quality or in good condition do not vary as much as you think they would throughout the late 1800’s and the early 1900’s. It is also surprisling comprehensive to build a basic AI model, and studying this project can teach you a lot about how you can develop your own AI models!\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "Client Report - Project 4",
    "section": "",
    "text": "Using AI, I was able to find specific features of homes that can accuratly predict whether a home was built before or after 1980. Some of those things were the number of bathrooms, the total number of living space in the home, and the architecture style of the home. You would be surprised to see how many homes that are rated as great quality or in good condition do not vary as much as you think they would throughout the late 1800’s and the early 1900’s. It is also surprisling comprehensive to build a basic AI model, and studying this project can teach you a lot about how you can develop your own AI models!\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#potential-feature-charts",
    "href": "Projects/project4.html#potential-feature-charts",
    "title": "Client Report - Project 4",
    "section": "Potential Feature Charts",
    "text": "Potential Feature Charts\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980.\nI used averages of data and also counts of data to find relationships between different features of homes to see which ones show the highest chance of the prediction model being the most accurate. The box plot I created showed the average living space of homes before and after 1980 and there were clear differences in both categories. I know this would help the model predict which homes are most likely built before 1980 since there was a great difference in the averages. I also included bar charts and the information I was looking for was the ratio of before and after 1980 data and kept the highest difference in the counts.\n\n\nRead and format data\n# Include and execute your code here\n\nwith sqlite3.connect('your_database.db') as con:\n    # Write DataFrame to SQLite\n    df.to_sql('home_data', con, index=False, if_exists='replace')\n\n    # Query and display results\n    q_livearea = '''\n    SELECT yrbuilt, AVG(livearea) AS AVG_livearea, before1980\n    FROM home_data\n    GROUP BY yrbuilt;\n    '''\n\n    q_quality = '''\n    SELECT before1980, 'Quality A' AS quality_type, SUM(quality_A) AS count\n    FROM home_data\n    GROUP BY quality_type, before1980\n\n    UNION ALL\n\n    SELECT before1980, 'Quality C' AS quality_type, SUM(quality_C) AS count\n    FROM home_data\n    GROUP BY quality_type, before1980\n\n    UNION ALL\n\n    SELECT before1980, 'Quality D' AS quality_type, SUM(quality_D) AS count\n    FROM home_data\n    GROUP BY quality_type, before1980;\n    '''\n\n    q_condition = '''\n\n    SELECT before1980, 'Very Good' AS condition, SUM(condition_VGood) AS count\n    FROM home_data\n    GROUP BY condition, before1980\n\n    UNION ALL\n\n    SELECT before1980, 'Good' AS condition, SUM(condition_Good) AS count\n    FROM home_data\n    GROUP BY condition, before1980;\n    '''\n\n    q_garage = '''\n\n    SELECT before1980, 'Detatched' AS garage_type, SUM(gartype_Det) AS count\n    FROM home_data\n    GROUP BY garage_type, before1980\n\n    UNION ALL\n\n    SELECT before1980, 'Car Port' AS garage_type, SUM(gartype_CP) AS count\n    FROM home_data\n    GROUP BY garage_type, before1980\n\n    UNION ALL\n\n    SELECT before1980, 'None' AS garage_type, SUM(gartype_None) AS count\n    FROM home_data\n    GROUP BY garage_type, before1980;\n    '''\n\n    q_arc = '''\n\n    SELECT before1980, 'CONVERSIONS' AS arc_style, SUM(\"arcstyle_CONVERSIONS\") AS count\n    FROM home_data\n    GROUP BY arc_style, before1980\n\n    UNION ALL\n\n    SELECT before1980, 'ONE AND HALF-STORY' AS arc_style, SUM(\"arcstyle_ONE AND HALF-STORY\") AS count\n    FROM home_data\n    GROUP BY arc_style, before1980\n\n    UNION ALL\n\n    SELECT before1980, 'ONE-STORY' AS arc_style, SUM(\"arcstyle_ONE-STORY\") AS count\n    FROM home_data\n    GROUP BY arc_style, before1980\n\n    UNION ALL\n\n    SELECT before1980, 'TWO-STORY' AS arc_style, SUM(\"arcstyle_TWO-STORY\") AS count\n    FROM home_data\n    GROUP BY arc_style, before1980;\n    '''\n    \n    result_livearea = pd.read_sql_query(q_livearea, con)\n    result_quality = pd.read_sql_query(q_quality, con)\n    result_condition = pd.read_sql_query(q_condition, con)\n    result_garage = pd.read_sql_query(q_garage, con)\n    result_arc = pd.read_sql_query(q_arc, con)\n\n\nThis is a box plot showing the differences in the living area of homes before and after 1980. The difference is clear and there is not much intersection.\n::: {#cell-Q1 box plot .cell execution_count=4}\n\nshow box plot\n# Include and execute your code here\n\nchart = px.box(\n    result_livearea,\n    x=\"before1980\",\n    y=\"AVG_livearea\",\n    labels={'AVG_livearea': 'Average Live Area', 'before1980': 'Before 1980'},\n    title='Living Area Before and After 1980'\n)\n\nchart.update_xaxes(tickvals=[0, 1], ticktext=['After 1980', 'Before 1980'])\n\nchart.show()\n\n\n                                                \nLiving Area Box Plot\n\n:::\nThese are all box plots that show the different counts of each of these ratings or features to compare them between before and after 1980.\n::: {#cell-Q1 chart A .cell execution_count=5}\n\nshow bar chart\n# Include and execute your code here\nchart = px.bar(\n    result_quality,\n    x=\"quality_type\",\n    y=\"count\",\n    color=\"before1980\",\n    barmode=\"group\",  # This ensures bars are grouped by \"before1980\"\n    labels={'count': 'Count', 'quality_type': 'Quality', 'before1980': 'Before 1980'},\n    title='Quality of Homes Before and After 1980'\n)\n\nchart.show()\n\n\n                                                \nQuality Bar Chart\n\n:::\n::: {#cell-Q1 chart B .cell execution_count=6}\n\nshow bar chart\n# Include and execute your code here\nchart = px.bar(\n    result_condition,\n    x=\"condition\",\n    y=\"count\",\n    color=\"before1980\",\n    barmode=\"group\",  # This ensures bars are grouped by \"before1980\"\n    labels={'count': 'Count', 'condition': 'Condition', 'before1980': 'Before 1980'},\n    title='Condition of Homes Before and After 1980'\n)\n\nchart.show()\n\n\n                                                \nCondition Bar Chart\n\n:::\n::: {#cell-Q1 chart C .cell execution_count=7}\n\nshow bar chart\n# Include and execute your code here\nchart = px.bar(\n    result_garage,\n    x=\"garage_type\",\n    y=\"count\",\n    color=\"before1980\",\n    barmode=\"group\",  # This ensures bars are grouped by \"before1980\"\n    labels={'count': 'Count', 'garage_type': 'Garage Type', 'before1980': 'Before 1980'},\n    title='Garage Types Before and After 1980'\n)\n\nchart.show()\n\n\n                                                \nGarage Type Bar Chart\n\n:::\n::: {#cell-Q1 chart D .cell execution_count=8}\n\nshow bar chart\n# Include and execute your code here\nchart = px.bar(\n    result_arc,\n    x=\"arc_style\",\n    y=\"count\",\n    color=\"before1980\",\n    barmode=\"group\",  # This ensures bars are grouped by \"before1980\"\n    labels={'count': 'Count', 'arc_style': 'Arc Style', 'before1980': 'Before 1980'},\n    title='Arc Styles Before and After 1980'\n)\n\nchart.show()\n\n\n                                                \nArc Bar Chart\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#random-forest-model",
    "href": "Projects/project4.html#random-forest-model",
    "title": "Client Report - Project 4",
    "section": "Random Forest Model",
    "text": "Random Forest Model\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\n_The algorithm that I found was best for the data that was provided was the Random Forest model. And I decided to use a wide varity of features with one of the most important ones being the living area of each home. I found that was a great feature to help the prediction models. I also tried to use a logistic regression, SVM, gradiant boosting, and KNN models to test out the data.\n\n\nRead and format data, prepare model data\n# Include and execute your code here\n# Read data\nX = df[[\n    'livearea', 'arcstyle_ONE-STORY', 'arcstyle_ONE AND HALF-STORY', 'gartype_Det', 'basement', 'totunits', 'stories', 'nocars', 'numbdrm' , 'numbaths'\n]]\n\ny = df['before1980']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n\nThis will print out different evaluation metrics like the accuracy, confusion matrix, and the Classification Report.\n\n\nCalculate accuracy, conf matrix, class rep\n# Include and execute your code here\n\n# Initialize the Random Forest model\nrf_model = RandomForestClassifier()\n\n# Train the model using the training data\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred_rf = rf_model.predict(X_test)\n\nconf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\nclassification_rep_rf = classification_report(y_test, y_pred_rf)\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\n\n\n\n\nRead and format data, prepare model data\n# Include and execute your code here\n# Read data\nX = df[[\n    'sprice'\n]]\n\ny = df['before1980']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=76)\n\naverage_first_10_values = X_train.head(10).mean()\n\nprint(\"Average of the first 10 values in testing y:\", average_first_10_values)\n\n\nAverage of the first 10 values in testing y: sprice    2131970.0\ndtype: float64\n\n\nThis will print out different evaluation metrics like the accuracy, confusion matrix, and the Classification Report.\n\n\nCalculate accuracy, conf matrix, class rep\n# Include and execute your code here\n\n# Initialize the Random Forest model\nrf_model = RandomForestClassifier()\n\n# Train the model using the training data\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred_rf = rf_model.predict(X_test)\n\nconf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\nclassification_rep_rf = classification_report(y_test, y_pred_rf)\naccuracy_rf = accuracy_score(y_test, y_pred_rf)",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#most-important-features",
    "href": "Projects/project4.html#most-important-features",
    "title": "Client Report - Project 4",
    "section": "Most Important Features",
    "text": "Most Important Features\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\nWhen trying to find the best features for my model in the section labeled “Potential Feature Charts” I found one of the best features was the living area. As I actually was feature engineering my model I noticed that a lot of the columns in the data that I thought would be most beneficial were not giving me the accuracy that I was looking for, so for this section I graphed the additional features that I found most beneficial for my model including units, stories, and basement. I shared the unit graph as a line graph because it is easy to see the differences throughout the years, but the box plots of the stories, and bathrooms show a obvious difference in the averages similar to the living area graph I mentioned earlier that cannot be seen on a line graph.\n\n\nRead and format data\n# Include and execute your code here\n\nwith sqlite3.connect('your_database.db') as con:\n    # Write DataFrame to SQLite\n    df.to_sql('home_data', con, index=False, if_exists='replace')\n\n    # Query and display results\n    q_units = '''\n    SELECT yrbuilt, AVG(totunits) AS AVG_units, before1980\n    FROM home_data\n    GROUP BY yrbuilt;\n    '''\n\n    q_stories = '''\n    SELECT yrbuilt, AVG(stories) AS AVG_stories, before1980\n    FROM home_data\n    GROUP BY yrbuilt;\n    '''\n\n    q_baths = '''\n    SELECT yrbuilt, AVG(stories) AS AVG_baths, before1980\n    FROM home_data\n    GROUP BY yrbuilt;\n    '''\n\n    result_units = pd.read_sql_query(q_units, con)\n    result_stories = pd.read_sql_query(q_stories, con)\n    result_baths = pd.read_sql_query(q_baths, con)\n\n\nLine graph showing the different number of units from 1873-2013.\n::: {#cell-Q3 Line Chart .cell execution_count=14}\n\nplot line chart\n# Include and execute your code here\nchart = px.line(result_units,\n    x=\"yrbuilt\", \n    y=\"AVG_units\",\n    labels={'AVG_units': 'Average Number of Units', 'yrbuilt': 'Year Built'},\n    title='Units per Home From 1800-2000'\n)\n\nchart.add_shape(\n    go.layout.Shape(\n        type=\"line\",\n        x0=1980,\n        x1=1980,\n        y0=0,\n        y1=result_units['AVG_units'].max(),  # Adjust the y1 value as needed\n        line=dict(color=\"red\", width=2, dash=\"dash\"),\n    )\n)\n\nchart.show()\n\n\n                                                \nLine Chart of Average Units\n\n:::\nBox plots showing the difference in means of stories, and bathrooms before and after 1980.\n::: {#cell-Q3 Box Plot A .cell execution_count=15}\n\nshow box plot\n# Include and execute your code here\nchart = px.box(\n    result_stories,\n    x=\"before1980\",\n    y=\"AVG_stories\",\n    labels={'AVG_stories': 'Average Number of Stories', 'before1980': 'Before 1980'},\n    title='Average Number of Stories Before and After 1980'\n)\n\nchart.update_xaxes(tickvals=[0, 1], ticktext=['After 1980', 'Before 1980'])\n\nchart.show()\n\n\n                                                \nBox Plot of Average Stories\n\n:::\n::: {#cell-Q3 Box Plot B .cell execution_count=16}\n\nshow box plot\n# Include and execute your code here\nchart = px.box(\n    result_baths,\n    x=\"before1980\",\n    y=\"AVG_baths\",\n    labels={'AVG_baths': 'Average Number of Baths', 'before1980': 'Before 1980'},\n    title='Average Number of Bathrooms Before and After 1980'\n)\n\nchart.update_xaxes(tickvals=[0, 1], ticktext=['After 1980', 'Before 1980'])\n\nchart.show()\n\n\n                                                \nBox Plots of Average Bathrooms\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#prediction-metrics",
    "href": "Projects/project4.html#prediction-metrics",
    "title": "Client Report - Project 4",
    "section": "Prediction Metrics",
    "text": "Prediction Metrics\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nI was able to make a model with 91% accuracy which means that after the model trained with 70% of the data the remaining 30% was tested and predictions were madde, the model could then predict if a home was built before or after 1980 with that accuracy. I also have the Confusion Matrix which shows the breakdown of where the model succeeded and failed in the positive or negative. There is a True Positive where the model accuratly predicted a home was built before 1980, True Negative where the model accuratly predicted a home was not built before 1980, False Positive where the model inaccuratly predicated a home was built before 1980, and finally False Negative where the model inaccuratly predicted a home was not built before 1980.\n\n\nShow Model Data\n# Include and execute your code here\nprint(\"Accuracy - Random Forest:\", accuracy_rf)\nprint('--------------------------------------------------------------')\nprint(\"Confusion Matrix - Random Forest:\\n\", conf_matrix_rf)\n\n\nAccuracy - Random Forest: 0.6560133487357207\n--------------------------------------------------------------\nConfusion Matrix - Random Forest:\n [[ 993 1891]\n [ 789 4118]]",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "projects.html#repo-for-all-my-projects",
    "href": "projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "Templates/DS250_Template.html",
    "href": "Templates/DS250_Template.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Uncomment the entire section to use this template\n\n\n\n\n\n Back to top"
  }
]